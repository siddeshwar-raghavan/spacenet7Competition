{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test the SpaceNet 7 Baseline Algorithm\n",
    "\n",
    "\n",
    "We assume that initial steps of README have been executed, sn7_data_prep.ipynb has been executed, and that this notebook is running in a docker container.  See the `src` directory for functions used in the algorithm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## 1. Train\n",
    "1. Launch a separate docker container for training:\n",
    "\n",
    "       nvidia-docker build -t sn7_baseline_image /path_to_baseline/docker \n",
    "       NV_GPU=0 nvidia-docker run -it -v /local_data:/local_data  -ti --ipc=host --name sn7_gpu0 sn7_baseline_image\n",
    "       conda activate solaris\n",
    "    \n",
    "2. Initiate training.  First edit `sn7_baseline_train.yml` to point to the correct data paths, then execute the following in the command line: \n",
    "    \n",
    "       cd /path_to_baseline/src\n",
    "       time python sn7_baseline_train.py\n",
    "\n",
    "    Training for the full 300 epochs should take 20 hours (~$60) on a p3.2xlarge AWS instance.\n",
    "    \n",
    " \n",
    " \n",
    "3. Alternately, instead of training, one could use the pre-trained weights included in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## 2. Infer\n",
    "Once training is completed (or pre-trained weights are selected) it's time to initiate inference. First edit `sn7_baseline_infer.yml` to point to the correct data paths, run:\n",
    "\n",
    "        cd /path_to_baseline/src\n",
    "        time python sn7_baseline_infer.py\n",
    "\n",
    "\n",
    "This script will execute in ~2.5 minutes on a p3.2xlarge AWS instance (which equates to  approximately 60 square kilometers per second)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## 3. Extract Footprints and Building Identifiers\n",
    "\n",
    "The `sn7_baseline_infer.py` script executes the segmentation model, which is only the first step in the extracting matched building footprints in the data cube.  In the cells below, we refine these predictioms masks to the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prediction and image directories (edit appropriately)\n",
    "pred_top_dir = '/path_to_baseline/inference_out/sn7_baseline_preds'\n",
    "im_top_dir = '/local_data/sn7/aws_download/test_public'\n",
    "\n",
    "from shapely.ops import cascaded_union\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import tqdm\n",
    "import glob\n",
    "import math\n",
    "import gdal\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg') # non-interactive\n",
    "\n",
    "import solaris as sol\n",
    "from solaris.utils.core import _check_gdf_load\n",
    "from solaris.raster.image import create_multiband_geotiff \n",
    "\n",
    "# import from data_postproc_funcs\n",
    "module_path = os.path.abspath(os.path.join('../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from sn7_baseline_postproc_funcs import map_wrapper, multithread_polys, \\\n",
    "        calculate_iou, track_footprint_identifiers, \\\n",
    "        sn7_convert_geojsons_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### 3.A. Group predictions by AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_name = 'raw/'\n",
    "grouped_name = 'grouped/'\n",
    "im_list = sorted([z for z in os.listdir(os.path.join(pred_top_dir, raw_name)) if z.endswith('.tif')])\n",
    "df = pd.DataFrame({'image': im_list})\n",
    "roots = [z.split('mosaic_')[-1].split('.tif')[0] for z in df['image'].values]\n",
    "df['root'] = roots\n",
    "# copy files\n",
    "for idx, row in df.iterrows():\n",
    "    in_path_tmp = os.path.join(pred_top_dir, raw_name, row['image'])\n",
    "    out_dir_tmp = os.path.join(pred_top_dir, grouped_name, row['root'], 'masks')\n",
    "    os.makedirs(out_dir_tmp, exist_ok=True)\n",
    "    cmd = 'cp ' + in_path_tmp + ' ' + out_dir_tmp\n",
    "    print(\"cmd:\", cmd)\n",
    "    os.system(cmd)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## 3.B. (Optional) Explore predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect visually\n",
    "\n",
    "aoi = 'L15-0509E-1108N_2037_3758_13'\n",
    "out_dir_explore = os.path.join(pred_top_dir, 'explore', aoi)\n",
    "os.makedirs(out_dir_explore, exist_ok=True)\n",
    "\n",
    "pred_dir = os.path.join(pred_top_dir, 'grouped', aoi, 'masks')\n",
    "im_dir = os.path.join(im_top_dir, aoi, 'images_masked')\n",
    "im_list = sorted([z for z in os.listdir(pred_dir) if z.endswith('.tif')])\n",
    "sample_mask_name = im_list[0]\n",
    "sample_mask_path = os.path.join(pred_dir, sample_mask_name)\n",
    "sample_im_path = os.path.join(im_dir, sample_mask_name)\n",
    "\n",
    "image = skimage.io.imread(sample_im_path)\n",
    "mask_image = skimage.io.imread(sample_mask_path)\n",
    "print(\"mask_image.shape:\", mask_image.shape)\n",
    "print(\"min, max, mean mask image:\", np.min(mask_image), np.max(mask_image), np.mean(mask_image))\n",
    "\n",
    "# # vertical layout\n",
    "# figsize = (14, 14)\n",
    "# fig, ax = plt.subplots(figsize=figsize)\n",
    "# _ = ax.imshow(image)\n",
    "# ax.set_title(sample_mask_name)\n",
    "# plt.savefig(os.path.join(out_dir_explore, aoi + '_im0.png'))\n",
    "# fig, ax = plt.subplots(figsize=figsize)\n",
    "# _ = ax.imshow(mask_image)\n",
    "# ax.set_title(sample_mask_name)\n",
    "# plt.savefig(os.path.join(out_dir_explore, aoi + '_mask0.png'))\n",
    "# plt.show()\n",
    "\n",
    "# horizontal \n",
    "figsize = (20, 10)\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=figsize)\n",
    "_ = ax0.imshow(image)\n",
    "ax0.set_xticks([])\n",
    "ax0.set_yticks([])\n",
    "ax0.set_title('Image')\n",
    "_ = ax1.imshow(mask_image)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('Prediction Mask')\n",
    "plt.suptitle(sample_mask_name.split('.')[0])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir_explore, aoi + '_im0+mask0.png'))\n",
    "plt.show()\n",
    "\n",
    "# zoom in a subset \n",
    "# horizontal \n",
    "figsize = (20, 10)\n",
    "bounds = []\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=figsize)\n",
    "_ = ax0.imshow(image[400:600,400:600,:])\n",
    "ax0.set_xticks([])\n",
    "ax0.set_yticks([])\n",
    "ax0.set_title('Image')\n",
    "_ = ax1.imshow(mask_image[400:600,400:600])\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('Prediction Mask')\n",
    "plt.suptitle(sample_mask_name.split('.')[0])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir_explore, aoi + '_im0+mask0_zoom.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and inspect sample footprints\n",
    "# https://solaris.readthedocs.io/en/latest/tutorials/notebooks/api_mask_to_vector.html\n",
    "# https://github.com/CosmiQ/solaris/blob/master/solaris/vector/mask.py#L718\n",
    "\n",
    "output_path_pred = os.path.join(out_dir_explore, aoi + '_pred0.geojson')\n",
    "min_area = 3.5    # in pixels\n",
    "bg_threshold = 0\n",
    "simplify = False\n",
    "print(\"bg_threshold:\", bg_threshold)\n",
    "print(\"min_area:\", min_area)\n",
    "geoms = sol.vector.mask.mask_to_poly_geojson(mask_image, \n",
    "                                             min_area=min_area, \n",
    "                                             output_path=output_path_pred,\n",
    "                                             output_type='geojson',\n",
    "                                             bg_threshold=bg_threshold,\n",
    "                                             simplify=simplify)\n",
    "display(geoms.head())\n",
    "print(\"N geoms:\", len(geoms))\n",
    "\n",
    "# get plot geoms\n",
    "plot_geoms = cascaded_union(geoms['geometry'])\n",
    "# display(plot_geoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each polygon shape\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "for geom in plot_geoms.geoms:\n",
    "    ax.plot(*geom.exterior.xy)\n",
    "ax.set_xlim(0, mask_image.shape[1])\n",
    "ax.set_ylim(mask_image.shape[1], 0)\n",
    "ax.set_aspect('equal')\n",
    "# Set (current) axis to be equal before showing plot?\n",
    "# plt.gca().axis(\"equal\")\n",
    "plt.savefig(os.path.join(out_dir_explore, aoi + '_footprints0.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## 3.C. Extract building footprint geometries for all AOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all geoms for all aois (mult-threaded)\n",
    "\n",
    "min_area = 3.5   # in pixels (4 is standard)\n",
    "simplify = False\n",
    "bg_threshold = 0  \n",
    "output_type = 'geojson'\n",
    "aois = sorted([f for f in os.listdir(os.path.join(pred_top_dir, 'grouped')) if os.path.isdir(os.path.join(pred_top_dir, 'grouped', f))])\n",
    "\n",
    "# set params\n",
    "params = []\n",
    "for i, aoi in enumerate(aois):\n",
    "    print(i, \"/\", len(aois), aoi)   \n",
    "    outdir = os.path.join(pred_top_dir, 'grouped', aoi, 'pred_jsons')\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    pred_files = sorted([os.path.join(pred_top_dir, 'grouped', aoi, 'masks', f)\n",
    "                for f in sorted(os.listdir(os.path.join(pred_top_dir, 'grouped', aoi, 'masks')))\n",
    "                if f.endswith('.tif')])\n",
    "    for j, p in enumerate(pred_files):\n",
    "        name = os.path.basename(p)\n",
    "        # print(i, j, name)\n",
    "        output_path_pred = os.path.join(outdir,  name.split('.tif')[0] + '.geojson')\n",
    "        # get pred geoms\n",
    "        if not os.path.exists(output_path_pred):\n",
    "            pred_image = skimage.io.imread(p)#[:,:,0]\n",
    "            params.append([pred_image, min_area, output_path_pred,\n",
    "                          output_type, bg_threshold, simplify])        \n",
    "\n",
    "print(\"Execute!\")\n",
    "print(\"len params:\", len(params))\n",
    "n_threads = 10\n",
    "pool = multiprocessing.Pool(n_threads)\n",
    "_ = pool.map(multithread_polys, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## 3.D. Track building identifiers\n",
    "\n",
    "Now we assign a unique identifier to each building, and propogate that identifier through the data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes awhile, so multi-thread it\n",
    "\n",
    "min_iou = 0.2\n",
    "iou_field = 'iou_score'\n",
    "id_field = 'Id'\n",
    "reverse_order = False\n",
    "verbose = True\n",
    "super_verbose = False\n",
    "n_threads = 10\n",
    "\n",
    "json_dir_name = 'pred_jsons/'\n",
    "out_dir_name = 'pred_jsons_match/'\n",
    "aois = sorted([f for f in os.listdir(os.path.join(pred_top_dir, 'grouped')) \n",
    "               if os.path.isdir(os.path.join(pred_top_dir, 'grouped', f))])\n",
    "print(\"aois:\", aois)\n",
    "\n",
    "print(\"Gather data for matching...\")\n",
    "params = []\n",
    "for aoi in aois:\n",
    "    print(aoi)\n",
    "    json_dir = os.path.join(pred_top_dir, 'grouped', aoi, json_dir_name)\n",
    "    out_dir = os.path.join(pred_top_dir, 'grouped', aoi, out_dir_name)\n",
    "    \n",
    "    # check if we started matching...\n",
    "    if os.path.exists(out_dir):\n",
    "        # print(\"  outdir exists:\", outdir)\n",
    "        json_files = sorted([f\n",
    "                for f in os.listdir(os.path.join(json_dir))\n",
    "                if f.endswith('.geojson') and os.path.exists(os.path.join(json_dir, f))])\n",
    "        out_files_tmp = sorted([z for z in os.listdir(out_dir) if z.endswith('.geojson')])\n",
    "        if len(out_files_tmp) > 0:\n",
    "            if len(out_files_tmp) == len(json_files):\n",
    "                print(\"Dir:\", os.path.basename(out_dir), \"N files:\", len(json_files), \n",
    "                      \"directory matching completed, skipping...\")\n",
    "                continue\n",
    "            elif len(out_files_tmp) != len(json_files):\n",
    "                # raise Exception(\"Incomplete matching in:\", out_dir, \"with N =\", len(out_files_tmp), \n",
    "                #                 \"files (should have N_gt =\", \n",
    "                #                 len(json_files), \"), need to purge this folder and restart matching!\")\n",
    "                print(\"Incomplete matching in:\", out_dir, \"with N =\", len(out_files_tmp), \n",
    "                                \"files (should have N_gt =\", \n",
    "                                len(json_files), \"), purging this folder and restarting matching!\")\n",
    "                purge_cmd = 'rm -r ' + out_dir\n",
    "                print(\"  purge_cmd:\", purge_cmd)\n",
    "                if len(out_dir) > 20:\n",
    "                    purge_cmd = 'rm -r ' + out_dir\n",
    "                else:\n",
    "                    raise Exception(\"out_dir too short, maybe deleting something unintentionally...\")\n",
    "                    break\n",
    "                os.system(purge_cmd)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    params.append([track_footprint_identifiers, json_dir,  out_dir, min_iou, \n",
    "                   iou_field, id_field, reverse_order, verbose, super_verbose])    \n",
    "\n",
    "print(\"Len params:\", len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Execute!\")\n",
    "n_threads = 10\n",
    "pool = multiprocessing.Pool(n_threads)\n",
    "_ = pool.map(map_wrapper, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plots (optional)\n",
    "\n",
    "# %matplotlib notebook\n",
    "im_pix_size_x, im_pix_size_y = 1024, 1024\n",
    "max_plots = 2\n",
    "label_font_size = 5\n",
    "figsize = (16, 16)\n",
    "\n",
    "aois = ['L15-1281E-1035N_5125_4049_13']\n",
    "print(\"aois:\", aois)\n",
    "\n",
    "count = 0\n",
    "for i, aoi in enumerate(aois):\n",
    "    print(\"\\n\")\n",
    "    print(i, \"aoi:\", aoi)\n",
    "    \n",
    "    json_files = sorted([f\n",
    "                for f in os.listdir(os.path.join(pred_top_dir, 'grouped', aoi, 'pred_jsons_match'))\n",
    "                if f.endswith('.geojson') and os.path.exists(os.path.join(pred_top_dir, 'grouped', aoi, 'pred_jsons_match', f))])\n",
    "    # take only the first and last?\n",
    "    # json_files = [json_files[0], json_files[-1]]\n",
    "    # plot \n",
    "    for j, f in enumerate(json_files):\n",
    "        if count >= max_plots:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "        print(i, j, f)\n",
    "        name_root = f.split('.')[0]\n",
    "        json_path = os.path.join(pred_top_dir, 'grouped', aoi, 'pred_jsons_match', f)\n",
    "        print(\"name_root:\", name_root)\n",
    "        # print(\"json_path:\", json_path)\n",
    "        gdf_pix = _check_gdf_load(json_path)\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        for _, row in gdf_pix.iterrows():\n",
    "            geom = row['geometry']\n",
    "            poly_id = row['Id']\n",
    "            x, y = geom.exterior.xy\n",
    "            cx, cy = np.array(geom.centroid.xy).astype(float)\n",
    "            # print(\"centroid:\", centroid)\n",
    "            ax.plot(x, y)\n",
    "            # poly id\n",
    "            ax.annotate(str(poly_id), xy=(cx, cy), ha='center', size=label_font_size)\n",
    "            # text_object = plt.annotate(label, xy=(x_values[i], y_values[i]), ha='center')\n",
    "            # ax.text(cx, cy, str(poly_id))\n",
    "        ax.set_xlim(0, im_pix_size_x)\n",
    "        ax.set_ylim(0, im_pix_size_y)\n",
    "        title = str(j) + \" - \" + name_root + \" - N buildings = \" + str(len(gdf_pix))\n",
    "        ax.set_title(title)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## 3.E. Make proposal CSV \n",
    "This is necessary for scoring with the [SCOT metric](https://github.com/CosmiQ/solaris/blob/master/solaris/eval/scot.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make proposal csv\n",
    "\n",
    "out_dir_csv = os.path.join(pred_top_dir, 'csvs')\n",
    "os.makedirs(out_dir_csv, exist_ok=True)\n",
    "prop_file = os.path.join(out_dir_csv, 'sn7_baseline_predictions.csv')\n",
    "\n",
    "aoi_dirs = sorted([os.path.join(pred_top_dir, 'grouped', aoi, 'pred_jsons_match') \\\n",
    "                   for aoi in os.listdir(os.path.join(pred_top_dir, 'grouped')) \\\n",
    "                   if os.path.isdir(os.path.join(pred_top_dir, 'grouped', aoi, 'pred_jsons_match'))])\n",
    "print(\"aoi_dirs:\", aoi_dirs)\n",
    "\n",
    "# Execute\n",
    "if not os.path.exists(prop_file):\n",
    "    net_df = sn7_convert_geojsons_to_csv(aoi_dirs, prop_file, 'proposal')\n",
    "\n",
    "print(\"prop_file:\", prop_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## 4. Conclusions\n",
    "\n",
    "The notebook above walks through all the steps to train and test a model that extracts building footprints with (hopefully) persistent unique identifiers (i.e. addresses) from a deep temporal stack of medium resolution satellite imagery.  \n",
    "\n",
    "The model proposed here achieves a [SCOT](https://github.com/CosmiQ/solaris/blob/master/solaris/eval/scot.py) score of 0.158 on the SpaceNet 7 test_public data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
